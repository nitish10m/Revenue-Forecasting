# -*- coding: utf-8 -*-
"""LSTM_Univariate.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1004OJZg00WhyesGexGdmClV8YgcDD7JT
"""

import pandas as pd
import numpy as np

#df_rev = pd.read_csv("/content/drive/My Drive/Revenue Forecast Project/Data/Revenue.csv")
df_rev = pd.read_csv("/content/drive/My Drive/Revenue Forecast Project/Data/Revenue_original.csv")

from google.colab import drive
drive.mount('/content/drive')

df_rev.head()

"""Data cleaning steps"""

# Create Report date from raw data

conditions =[(df_rev['Quarter'].str.split(' ').str[1] =="Q1"),\
             (df_rev['Quarter'].str.split(' ').str[1] =="Q2"),\
             (df_rev['Quarter'].str.split(' ').str[1] =="Q3"),\
             (df_rev['Quarter'].str.split(' ').str[1] =="Q4")\
            ]

choices =[("31-03-" + df_rev['Quarter'].str.split(' ').str[0]),\
          ("30-06-" + df_rev['Quarter'].str.split(' ').str[0]),\
          ("30-09-" + df_rev['Quarter'].str.split(' ').str[0]),\
          ("31-12-" + df_rev['Quarter'].str.split(' ').str[0])\
         ]


df_rev['ReportDate'] = np.select(conditions, choices, default="NA")

df_rev.head()

#Convert Revernue to float type
df_rev['Revenue'] = df_rev['Revenue'].str.replace(',','')
df_rev['Revenue'] = df_rev['Revenue'].astype('float')

# Convert ReportDate to date type

df_rev['ReportDate'] = pd.to_datetime(df_rev['ReportDate'])

# remove unwanted column
df_rev.drop(['Quarter'], axis=1, inplace=True)

# sort the dataframe in acceding 
df_rev = df_rev.sort_values(['ReportDate'], ascending=[True])

# set column ReportDate as index of dataframe
df_rev.set_index('ReportDate', inplace=True)

# set column 'ReportDate' as Index of dataframe 
df_rev.head(5)
#temp = df_rev['Revenue']

# Reference doc
# https://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/

# This is based on LSTM implementation using Keras librarry
# The Long Short-Term Memory network, or LSTM network, is a recurrent neural network 
# that is trained using Backpropagation Through Time and overcomes the vanishing gradient problem.

# As such, it can be used to create large recurrent networks that in turn can be used to address 
# difficult sequence problems in machine learning and achieve state-of-the-art results.

# Instead of neurons, LSTM networks have memory blocks that are connected through layers.

# LSTMs can be used to model univariate time series forecasting problems.
# These are problems comprised of a single series of observations and a model is required to learn from 
# the series of past observations to predict the next value in the sequence.

# univariate lstm example
import numpy as np
import matplotlib.pyplot as plt
from pandas import read_csv
import math
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import LSTM
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
# convert an array of values into a dataset matrix

def create_dataset(dataset, look_back=1):
	dataX, dataY = [], []
	for i in range(len(dataset)-look_back-1):
		a = dataset[i:(i+look_back), 0]
		dataX.append(a)
		dataY.append(dataset[i + look_back, 0])
	return np.array(dataX), np.array(dataY)

# fix random seed for reproducibility
np.random.seed(7)
# normalize the dataset
scaler = MinMaxScaler(feature_range=(0, 1))
df_rev = scaler.fit_transform(df_rev)
# split into train and test sets
train_size = int(len(df_rev) * 0.67)
test_size = len(df_rev) - train_size
train, test = df_rev[0:train_size,:], df_rev[train_size:len(df_rev),:]
# reshape into X=t and Y=t+1
look_back = 3
trainX, trainY = create_dataset(train, look_back)
testX, testY = create_dataset(test, look_back)
# reshape input to be [samples, time steps, features]
trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))
testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))

# create and fit the LSTM network
# The network has a visible layer with 1 input with 4 LSTM blocks, a hidden layer with 4 LSTM blocks or neurons, 
# and an output layer that makes a single value prediction. 
# The network is trained for 2 epochs and a batch size of 1 is used.
# Here, We can play aorund with different hyperparameters and optimization techniques. 

batch_size = 1
model = Sequential()
model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))
model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))
model.add(Dense(1))
model.compile(loss='mean_squared_error', optimizer='adam')
for i in range(100):
	model.fit(trainX, trainY, epochs=2, batch_size=batch_size, verbose=2, shuffle=False)
	model.reset_states()

# Once the model is fit, we can estimate the performance of the model on the train and test datasets. 
# This will give us a point of comparison for new models.
# Note that we invert the predictions before calculating error scores to ensure that performance is reported in the same units as the original data.

# Finally, we can generate predictions using the model for both the train and test dataset 
# to get a visual indication of the skill of the model.

# make predictions
trainPredict = model.predict(trainX, batch_size=batch_size)
model.reset_states()
testPredict = model.predict(testX, batch_size=batch_size)
# invert predictions
trainPredict = scaler.inverse_transform(trainPredict)
trainY = scaler.inverse_transform([trainY])
testPredict = scaler.inverse_transform(testPredict)
testY = scaler.inverse_transform([testY])

# calculate root mean squared error
trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))
print('Train RMSE Score: %.2f ' % (trainScore))
testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))
print('Test RMSE Score: %.2f ' % (testScore))

# shift train predictions for plotting
trainPredictPlot = np.empty_like(df_rev)
trainPredictPlot[:, :] = np.nan
trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict
# shift test predictions for plotting
testPredictPlot = np.empty_like(df_rev)
testPredictPlot[:, :] = np.nan
testPredictPlot[len(trainPredict)+(look_back*2)+1:len(df_rev)-1, :] = testPredict

# plot baseline and predictions 
plt.plot(scaler.inverse_transform(df_rev),label="Original DataSet")
plt.plot(testPredictPlot, label= "Prediction for Unseen Test Dataset")
plt.plot(trainPredictPlot, label="Prediction for Trained DataSet ")
plt.legend()
plt.show()

# Univariate LSTM Models   
# https://machinelearningmastery.com/how-to-develop-lstm-models-for-time-series-forecasting/
#Vanilla LSTM

# 2nd way

# univariate lstm example
from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
 
# split a univariate sequence into samples
def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
# define input sequence
#raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]
# choose a number of time steps
n_steps = 3
# split into samples
X, y = split_sequence(df_rev, n_steps)
# reshape from [samples, timesteps] into [samples, timesteps, features]
n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features))
# define model
model = Sequential()
model.add(LSTM(50, activation='relu', input_shape=(n_steps, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=200, verbose=0)
# demonstrate prediction
x_input = array([70, 80, 90])
x_input = x_input.reshape((1, n_steps, n_features))
yhat = model.predict(x_input, verbose=0)
print(yhat)

# 3rd Way
#Stacked LSTM

# univariate stacked lstm example
from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
 
# split a univariate sequence
def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
# define input sequence
n_steps = 3
# split into samples
X, y = split_sequence(df_rev, n_steps)
# reshape from [samples, timesteps] into [samples, timesteps, features]
n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features))
# define model
model = Sequential()
model.add(LSTM(50, activation='relu', return_sequences=True, input_shape=(n_steps, n_features)))
model.add(LSTM(50, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=200, verbose=0)
# demonstrate prediction
x_input = array([70, 80, 90])
x_input = x_input.reshape((1, n_steps, n_features))
yhat = model.predict(x_input, verbose=0)
print(yhat)

# Bidirectional LSTM
# 4th Way

# univariate bidirectional lstm example
from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Bidirectional
 
# split a univariate sequence
def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
# define input sequence
# choose a number of time steps
n_steps = 3
# split into samples
X, y = split_sequence(df_rev, n_steps)
# reshape from [samples, timesteps] into [samples, timesteps, features]
n_features = 1
X = X.reshape((X.shape[0], X.shape[1], n_features))
# define model
model = Sequential()
model.add(Bidirectional(LSTM(50, activation='relu'), input_shape=(n_steps, n_features)))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=200, verbose=0)
# demonstrate prediction
x_input = array([70, 80, 90])
x_input = x_input.reshape((1, n_steps, n_features))
yhat = model.predict(x_input, verbose=0)
print(yhat)

# CNN LSTM
# 5th Way

# univariate cnn lstm example
from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import TimeDistributed
from keras.layers.convolutional import Conv1D
from keras.layers.convolutional import MaxPooling1D
 
# split a univariate sequence into samples
def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
# define input sequence
n_steps = 4
# split into samples
X, y = split_sequence(df_rev, n_steps)
# reshape from [samples, timesteps] into [samples, subsequences, timesteps, features]
n_features = 1
n_seq = 2
n_steps = 2
X = X.reshape((X.shape[0], n_seq, n_steps, n_features))
# define model
model = Sequential()
model.add(TimeDistributed(Conv1D(filters=64, kernel_size=1, activation='relu'), input_shape=(None, n_steps, n_features)))
model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
model.add(TimeDistributed(Flatten()))
model.add(LSTM(50, activation='relu'))
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=500, verbose=0)
# demonstrate prediction
x_input = array([60, 70, 80, 90])
x_input = x_input.reshape((1, n_seq, n_steps, n_features))
yhat = model.predict(x_input, verbose=0)
print(yhat)

# ConvLSTM

# univariate convlstm example
from numpy import array
from keras.models import Sequential
from keras.layers import LSTM
from keras.layers import Dense
from keras.layers import Flatten
from keras.layers import ConvLSTM2D
 
# split a univariate sequence into samples
def split_sequence(sequence, n_steps):
	X, y = list(), list()
	for i in range(len(sequence)):
		# find the end of this pattern
		end_ix = i + n_steps
		# check if we are beyond the sequence
		if end_ix > len(sequence)-1:
			break
		# gather input and output parts of the pattern
		seq_x, seq_y = sequence[i:end_ix], sequence[end_ix]
		X.append(seq_x)
		y.append(seq_y)
	return array(X), array(y)
 
# define input sequence
# choose a number of time steps
n_steps = 4
# split into samples
X, y = split_sequence(df_rev, n_steps)
# reshape from [samples, timesteps] into [samples, timesteps, rows, columns, features]
n_features = 1
n_seq = 2
n_steps = 2
X = X.reshape((X.shape[0], n_seq, 1, n_steps, n_features))
# define model
model = Sequential()
model.add(ConvLSTM2D(filters=64, kernel_size=(1,2), activation='relu', input_shape=(n_seq, 1, n_steps, n_features)))
model.add(Flatten())
model.add(Dense(1))
model.compile(optimizer='adam', loss='mse')
# fit model
model.fit(X, y, epochs=500, verbose=0)
# demonstrate prediction
x_input = array([60, 70, 80, 90])
x_input = x_input.reshape((1, n_seq, 1, n_steps, n_features))
yhat = model.predict(x_input, verbose=0)
print(yhat)

